{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning intro\n",
    "\n",
    "![](https://newapplift-production.s3.amazonaws.com/comfy/cms/files/files/000/001/201/original/machine-learning-robots-dilbert.gif) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Training and quality assessment of the model is carried out on independent sets of examples. As a rule, existing examples are divided into two subsets: training (train) and test (test). The choice of split ratio is a compromise. Indeed, the large size of the training leads to better algorithms, but more noise when evaluating the model on the test. Conversely, a large test sample size leads to a less noisy quality assessment, but the trained models are less accurate.\n",
    "\n",
    "Many classification models predict a rating of membership to a positive class $ \\tilde {y} (x) \\in R $ (for example, the probability of being a class 1). After that, a decision is made on the class of the object by comparing the estimate with a certain threshold $ \\ theta $:\n",
    "\n",
    "$$y(x) = \n",
    "\\begin{cases}\n",
    "+1, &\\text{if} \\; \\tilde{y}(x) \\geq \\theta \\\\\n",
    "-1, &\\text{if} \\; \\tilde{y}(x) < \\theta\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In this case, we can consider metrics that can work with the initial response of the classifier. In the assignment, we will work with the AUC-ROC metric, which in this case can be considered as the proportion of incorrectly ordered pairs of objects sorted by ascending the predicted grade 1 grade. A detailed understanding of how the AUC-ROC metrics work for this lab is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of hyperparameters of the model\n",
    "\n",
    "In machine learning tasks, one should distinguish between model parameters and hyperparameters (structural parameters). Typically, model parameters are adjusted during training (for example, weights in a linear model or a decision tree structure), while hyper parameters are set in advance (for example, the value of the regularization force in a linear model or the maximum depth of a decision tree). Each model, as a rule, has a lot of hyperparameters and there are no universal sets of hyperparameters that work optimally in all tasks, so for each task you need to choose your own set.\n",
    "\n",
    "To optimize the hyperparameters, models often use __search on the grid (grid search)__: for each hyperparameter, several values are selected, then all combinations of values are selected and a combination is selected on which the model shows the best quality (in terms of the metric being optimized). However, in this case, it is necessary to correctly evaluate the constructed model, namely, to do the partition into a training and test sample. There are several schemes for how this can be implemented:\n",
    "\n",
    " - Break the available sample into training and test. In this case, the comparison of a large number of models in the enumeration of hyperparameters leads to a situation where the best model on the test subsample does not retain its qualities on the new data. We can say that there is a _transition_ on a test sample.\n",
    " - To eliminate the problem described above, you can split the data into 3 non-overlapping subsamples: training, validation, and test. Validation subsample is used to compare models, and test - for the final quality assessment and comparison of families of models with selected hyperparameters.\n",
    " - Another way to compare models is [cross-validation] (http://bit.ly/1CHXsNH). There are various cross-validation schemes:\n",
    "  - Leave-One-Out\n",
    "  - K-Fold\n",
    "  - Repeated random sampling\n",
    "  \n",
    "Cross validation is computationally expensive, especially if you are iterating over a grid with a very large number of combinations. Given the finiteness of the time to perform the task, a number of compromises arise: \n",
    "  - the hyperparameter grid can be made more sparse by going through fewer values of each hyperparameter; however, you should not forget that in this case you can skip a good combination of hyperparameters;\n",
    "  - cross-validation can be done with a smaller number of splits or folds, but in this case the quality assessment becomes more noisy and the risk of choosing a non-optimal set of hyperparameters increases due to randomness of splitting\n",
    "  - hyperparameters can be optimized sequentially (greedily) - one by one, and not to go through all the combinations; such a strategy does not always lead to an optimal set;\n",
    "  - sort through not all combinations of hyperparameters, but a small number of randomly selected ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn to teach machine learning models, correctly set up experiments, select hyperparameters, compare and mix models.\n",
    "\n",
    "You are asked to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form (not scaled) and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type.\n",
    "\n",
    "This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.\n",
    " \n",
    "The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n",
    "\n",
    "1. Spruce/Fir\n",
    "2. Lodgepole Pine\n",
    "3. Ponderosa Pine\n",
    "4. Cottonwood/Willow\n",
    "5. Aspen\n",
    "6. Douglas-fir\n",
    "7. Krummholz\n",
    "\n",
    "The training set (11340 observations) contains both features and the Cover_Type.\n",
    "\n",
    "# Data Fields\n",
    "__Elevation__ - Elevation in meters  \n",
    "__Aspect__ - Aspect in degrees azimuth  \n",
    "__Slope__ - Slope in degrees  \n",
    "__Horizontal_Distance_To_Hydrology__ - Horz Dist to nearest surface water features  \n",
    "__Vertical_Distance_To_Hydrology__ - Vert Dist to nearest surface water features  \n",
    "__Horizontal_Distance_To_Roadways__ - Horz Dist to nearest roadway  \n",
    "__Hillshade_9am__ (0 to 255 index) - Hillshade index at 9am, summer solstice  \n",
    "__Hillshade_Noon__ (0 to 255 index) - Hillshade index at noon, summer solstice  \n",
    "__Hillshade_3pm__ (0 to 255 index) - Hillshade index at 3pm, summer solstice  \n",
    "__Horizontal_Distance_To_Fire_Points__ - Horz Dist to nearest wildfire ignition points  \n",
    "__Wilderness_Area__ - Wilderness area designation  \n",
    "__Soil_Type__ - Soil Type designation  \n",
    "__Cover_Type__ (7 types, integers 1 to 7) - Forest Cover Type designation  \n",
    "\n",
    "The wilderness areas are:\n",
    "\n",
    "1. Rawah Wilderness Area\n",
    "2. Neota Wilderness Area\n",
    "3. Comanche Peak Wilderness Area\n",
    "4. Cache la Poudre Wilderness Area\n",
    "\n",
    "The soil types are:\n",
    "\n",
    "1. Cathedral family - Rock outcrop complex, extremely stony.\n",
    "2. Vanet - Ratake families complex, very stony.\n",
    "3. Haploborolis - Rock outcrop complex, rubbly.\n",
    "4. Ratake family - Rock outcrop complex, rubbly.\n",
    "5. Vanet family - Rock outcrop complex complex, rubbly.\n",
    "6. Vanet - Wetmore families - Rock outcrop complex, stony.\n",
    "7. Gothic family.\n",
    "8. Supervisor - Limber families complex.\n",
    "9. Troutville family, very stony.\n",
    "10. Bullwark - Catamount families - Rock outcrop complex, rubbly.\n",
    "11. Bullwark - Catamount families - Rock land complex, rubbly.\n",
    "12. Legault family - Rock land complex, stony.\n",
    "13. Catamount family - Rock land - Bullwark family complex, rubbly.\n",
    "14. Pachic Argiborolis - Aquolis complex.\n",
    "15. unspecified in the USFS Soil and ELU Survey.\n",
    "16. Cryaquolis - Cryoborolis complex.\n",
    "17. Gateview family - Cryaquolis complex.\n",
    "18. Rogert family, very stony.\n",
    "19. Typic Cryaquolis - Borohemists complex.\n",
    "20. Typic Cryaquepts - Typic Cryaquolls complex.\n",
    "21. Typic Cryaquolls - Leighcan family, till substratum complex.\n",
    "22. Leighcan family, till substratum, extremely bouldery.\n",
    "23. Leighcan family, till substratum - Typic Cryaquolls complex.\n",
    "24. Leighcan family, extremely stony.\n",
    "25. Leighcan family, warm, extremely stony.\n",
    "26. Granile - Catamount families complex, very stony.\n",
    "27. Leighcan family, warm - Rock outcrop complex, extremely stony.\n",
    "28. Leighcan family - Rock outcrop complex, extremely stony.\n",
    "29. Como - Legault families complex, extremely stony.\n",
    "30. Como family - Rock land - Legault family complex, extremely stony.\n",
    "31. Leighcan - Catamount families complex, extremely stony.\n",
    "32. Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n",
    "33. Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n",
    "34. Cryorthents - Rock land complex, extremely stony.\n",
    "35. Cryumbrepts - Rock outcrop - Cryaquepts complex.\n",
    "36. Bross family - Rock land - Cryumbrepts complex, extremely stony.\n",
    "37. Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n",
    "38. Leighcan - Moran families - Cryaquolls complex, extremely stony.\n",
    "39. Moran family - Cryorthents - Leighcan family complex, extremely stony.\n",
    "40. Moran family - Cryorthents - Rock land complex, extremely stony.\n",
    "\n",
    "Download the data set **forest_train.csv**. To better understand what you are working with / whether you have correctly loaded the data, you can display the first few lines on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Soil_Type</th>\n",
       "      <th>Wilderness_Area</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2340</td>\n",
       "      <td>321</td>\n",
       "      <td>13</td>\n",
       "      <td>272</td>\n",
       "      <td>130</td>\n",
       "      <td>984</td>\n",
       "      <td>185</td>\n",
       "      <td>227</td>\n",
       "      <td>181</td>\n",
       "      <td>752</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3003</td>\n",
       "      <td>315</td>\n",
       "      <td>16</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>3408</td>\n",
       "      <td>176</td>\n",
       "      <td>226</td>\n",
       "      <td>189</td>\n",
       "      <td>1126</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2912</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>306</td>\n",
       "      <td>102</td>\n",
       "      <td>306</td>\n",
       "      <td>220</td>\n",
       "      <td>212</td>\n",
       "      <td>124</td>\n",
       "      <td>607</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3292</td>\n",
       "      <td>285</td>\n",
       "      <td>29</td>\n",
       "      <td>999</td>\n",
       "      <td>194</td>\n",
       "      <td>3695</td>\n",
       "      <td>129</td>\n",
       "      <td>225</td>\n",
       "      <td>230</td>\n",
       "      <td>2356</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2588</td>\n",
       "      <td>247</td>\n",
       "      <td>4</td>\n",
       "      <td>212</td>\n",
       "      <td>-8</td>\n",
       "      <td>300</td>\n",
       "      <td>212</td>\n",
       "      <td>242</td>\n",
       "      <td>168</td>\n",
       "      <td>6186</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0           0       2340     321     13                               272   \n",
       "1           1       3003     315     16                                67   \n",
       "2           2       2912      39     13                               306   \n",
       "3           3       3292     285     29                               999   \n",
       "4           4       2588     247      4                               212   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                             130                              984   \n",
       "1                               0                             3408   \n",
       "2                             102                              306   \n",
       "3                             194                             3695   \n",
       "4                              -8                              300   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0            185             227            181   \n",
       "1            176             226            189   \n",
       "2            220             212            124   \n",
       "3            129             225            230   \n",
       "4            212             242            168   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points  Soil_Type  Wilderness_Area  Cover_Type  \n",
       "0                                 752          5                4           6  \n",
       "1                                1126         22                3           1  \n",
       "2                                 607         29                1           2  \n",
       "3                                2356         27                3           1  \n",
       "4                                6186         29                1           5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./forest_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes there are gaps in the data. The way to designate gaps is either prescribed in the data description, or at the skip place after reading the data, the value [NaN] appears (https://docs.scipy.org/doc/numpy-1.13.0/user/misc.html). For more information about working with passes in Pandas, you can read for example [here] (http://pandas.pydata.org/pandas-docs/stable/missing_data.html).\n",
    "In this dataset, missing values are marked with a \"?\".\n",
    "\n",
    "**(1 point) Task 1.** Usually, after loading a dataset, some preprocessing is always necessary. In this case, it will be as follows:\n",
    " - Find all the features that have missing values. Remove from the sample all objects with gaps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Save the target variable (the one we want to predict) into a separate variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df.Cover_Type)\n",
    "\n",
    "X = # Select only the columns that are not target here.\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - In this part we will work on binary classification problem. That is why we will select only the examples with Cover Type 1 and 2. Next we will classify examples with Cover Type 1 vs examples with Cover Type 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bin = X[(y == 1) | (y == 2)]\n",
    "y_bin = y[(y == 1) | (y == 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Please note that not all features are real (numeric). In the beginning we will work only with real features. Save them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3003</td>\n",
       "      <td>315</td>\n",
       "      <td>16</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>3408</td>\n",
       "      <td>176</td>\n",
       "      <td>226</td>\n",
       "      <td>189</td>\n",
       "      <td>1126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2912</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>306</td>\n",
       "      <td>102</td>\n",
       "      <td>306</td>\n",
       "      <td>220</td>\n",
       "      <td>212</td>\n",
       "      <td>124</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3292</td>\n",
       "      <td>285</td>\n",
       "      <td>29</td>\n",
       "      <td>999</td>\n",
       "      <td>194</td>\n",
       "      <td>3695</td>\n",
       "      <td>129</td>\n",
       "      <td>225</td>\n",
       "      <td>230</td>\n",
       "      <td>2356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3236</td>\n",
       "      <td>252</td>\n",
       "      <td>9</td>\n",
       "      <td>324</td>\n",
       "      <td>17</td>\n",
       "      <td>920</td>\n",
       "      <td>199</td>\n",
       "      <td>247</td>\n",
       "      <td>186</td>\n",
       "      <td>2310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3199</td>\n",
       "      <td>268</td>\n",
       "      <td>6</td>\n",
       "      <td>216</td>\n",
       "      <td>13</td>\n",
       "      <td>5918</td>\n",
       "      <td>204</td>\n",
       "      <td>243</td>\n",
       "      <td>178</td>\n",
       "      <td>1736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "1        3003     315     16                                67   \n",
       "2        2912      39     13                               306   \n",
       "3        3292     285     29                               999   \n",
       "7        3236     252      9                               324   \n",
       "13       3199     268      6                               216   \n",
       "\n",
       "    Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "1                                0                             3408   \n",
       "2                              102                              306   \n",
       "3                              194                             3695   \n",
       "7                               17                              920   \n",
       "13                              13                             5918   \n",
       "\n",
       "    Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "1             176             226            189   \n",
       "2             220             212            124   \n",
       "3             129             225            230   \n",
       "7             199             247            186   \n",
       "13            204             243            178   \n",
       "\n",
       "    Horizontal_Distance_To_Fire_Points  \n",
       "1                                 1126  \n",
       "2                                  607  \n",
       "3                                 2356  \n",
       "7                                 2310  \n",
       "13                                1736  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = [\n",
    "# Select only the columns that are numerical here.\n",
    "]\n",
    "\n",
    "num_features = X_bin[numeric_cols] # Select only numeric features here\n",
    "num_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 points) Training classifiers on real features\n",
    "\n",
    "In this section, it will be necessary to work only with real attributes and the target variable.\n",
    "\n",
    "In the beginning we will look at how the selection of hyperparameters on the grid works and how the quality of the partitioning affects the quality. Now and further we will consider 4 algorithms:\n",
    " - [kNN](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    " - [DecisonTree](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    " - [SGD Linear Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    " - [RandomForest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "To begin with, the first three algorithms will choose one hyperparameter, which we will optimize:\n",
    "  - kNN - number of neighbors (* n_neighbors *)\n",
    "  - DecisonTree - tree depth (* max_depth *)\n",
    "  - SGD Linear Classifier - optimized function (* loss *)\n",
    " \n",
    "Leave the default values for the remaining hyperparameters. For the selection of hyperparameters, use the search on the grid, which is implemented in the class [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). As a cross-validation scheme, use 5-Fold CV, which can be set using the class [KFoldCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold).\n",
    "\n",
    "![](https://i.stack.imgur.com/YWgro.gif)\n",
    "\n",
    "\n",
    "\n",
    "**(1.5 points) Task 2.** For each algorithm, select the optimal values of the specified hyperparameters. Build a graph of the average value of the quality of the cross-validation algorithm for a given value of the hyperparameter, which also display the confidence interval.\n",
    "\n",
    "To obtain the value of quality on each fold, the average value of quality and other useful information, you can use the field [*cv results_*](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "\n",
    "Which algorithm has the highest average quality ?\n",
    "\n",
    "Largest confidence interval ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert value that should be checked during the grid search of optimal parameters\n",
    "classifiers = {\n",
    "    KNeighborsClassifier:   [{'n_neighbors': [<Student, change me>]}],\n",
    "    DecisionTreeClassifier: [{'max_depth'  : [<Student, change me>]}],\n",
    "    SGDClassifier:          [{'loss'       : [<Student, change me>]}]\n",
    "}\n",
    "\n",
    "trained_clfs = []\n",
    "\n",
    "for classifier, params in classifiers.items():\n",
    "    clf = GridSearchCV(classifier(), param_grid=params, cv=KFold(n_splits=5), return_train_score=True)\n",
    "    clf.fit(num_features.values, y_bin)\n",
    "    trained_clfs.append([clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "mean_values = []\n",
    "stds = []\n",
    "names = []\n",
    "\n",
    "for clf in trained_clfs:\n",
    "    mean_vals = clf[0].cv_results_['mean_test_score']\n",
    "    std_vals = clf[0].cv_results_['std_test_score']\n",
    "    params = clf[0].cv_results_['params'][0]\n",
    "    \n",
    "    idx = np.argmin(clf[0].cv_results_['rank_test_score'])\n",
    "    opt_params = clf[0].cv_results_['params'][idx]\n",
    "    mean = clf[0].cv_results_['mean_test_score'][idx]\n",
    "    std = clf[0].cv_results_['std_test_score'][idx]\n",
    "    name = clf[0].estimator.__str__().split('(')[0] + ' , {}'.format(opt_params)\n",
    "    stds.append(std)\n",
    "    mean_values.append(mean)\n",
    "    names.append(name)\n",
    "\n",
    "plt.title('Performance of classifiers with the best optimized parameter value')\n",
    "plt.errorbar(np.arange(len(mean_values)), mean_values, stds, linestyle='None', marker='*')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('classifier')\n",
    "plt.xticks(np.arange(len(mean_values)), names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 point) Task 3.** Now let's select the number of trees (**n_estimators**) in the RandomForest algorithm. As you might know, in general, Random Forest does not overfit with an increase in the number of trees. Pick the number of trees starting from which the quality on cross-validation stabilizes. Note that to conduct this experiment, it is not necessary to train many random forests with different numbers of trees from scratch: train one random forest with the maximum interesting number of trees, and then consider subsets of trees of different sizes consisting of trees of the constructed forest (field [**estimators_**](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)). In further experiments, use the number of trees found.\n",
    "\n",
    "It is difficult to apply the **GridSearchCV** class in this task, so it is suggested to write a cycle on the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "clfs = []\n",
    "\n",
    "for train_index, test_index in kf.split(num_features.values):\n",
    "    X_train, X_test = num_features.values[train_index], num_features.values[test_index]\n",
    "    y_train, y_test = y_bin[train_index], y_bin[test_index]\n",
    "    \n",
    "    clf = <Create random forest classifier here>\n",
    "    <Train random forest classifier here>\n",
    "    \n",
    "    clfs.append([clf, X_test, y_test - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_fold = []\n",
    "\n",
    "for clf_n_data in clfs:\n",
    "    clf, x, y = clf_n_data\n",
    "    predictions = []\n",
    "    \n",
    "    for estimator in clf.estimators_:\n",
    "        predictions.append(<Make prediction here>)\n",
    "        \n",
    "    scores_by_fold.append([np.array(predictions), y])\n",
    "    \n",
    "quality_by_num_trees = []\n",
    "for num_trees in range(1, 301):\n",
    "    quality = []\n",
    "    \n",
    "    for predictions, labels in scores_by_fold:\n",
    "        vote = (predictions[:num_trees, :].mean(axis=0) > 0.5).astype(np.int64)\n",
    "        quality.append((vote == labels).sum() / float(len(labels)))\n",
    "        \n",
    "    quality_by_num_trees.append(quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "xs = []\n",
    "for i, num_trees in enumerate(range(1, 301)):\n",
    "    means.append(np.mean(quality_by_num_trees[i]))\n",
    "    stds.append(np.std(quality_by_num_trees[i]))\n",
    "    xs.append(num_trees)\n",
    "    \n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(xs, means, 'b', label='accuracy')\n",
    "plt.title('Avg quality by num_trees')\n",
    "plt.xlabel('num_trees')\n",
    "plt.xlabel('accuracy')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.fill_between(xs, np.array(means) - np.array(stds), np.array(means) + np.array(stds), facecolor='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When learning algorithms, it is worth paying attention not only to their quality, but also how they work with data. In this problem, it turned out that some of the algorithms used are sensitive to the scale of features. To make sure that this could affect the quality, let's look at the meanings of the attributes themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point) Task 4.** Look at the values of the features **Aspect**, **Slope**, **Hillshade_9am**. What is the feature of the data? Which of the considered algorithms can affect this? Can scaling affect the operation of these algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist(np.array(num_features.Elevation))\n",
    "plt.yscale('log')\n",
    "plt.ylabel('freqs')\n",
    "plt.xlabel('Elevation')\n",
    "plt.title('Elevation feature values distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can scale attributes, for example, in one of the following ways:\n",
    " - $x_{new} = \\dfrac{x - \\mu}{\\sigma}$, where $\\mu, \\sigma$ — the mean and standard deviation of the characteristic value over the entire sample (see the function [scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html))\n",
    " - $x_{new} = \\dfrac{x - x_{min}}{x_{max} - x_{min}}$, where $[x_{min}, x_{max}]$ — minimum interval of characteristic values\n",
    "\n",
    "Similar scaling schemes are given in classes [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) and [MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler).\n",
    " \n",
    "**(1 point) Task 5.** \n",
    "\n",
    "Scale all numerical attributes using one of the above methods and select the optimal values of the hyperparameters in the same way as above.\n",
    "\n",
    "Has the quality of some algorithms changed and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = <Scale features here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "<Compute and plot average quality by num trees with normalised features here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.5 point) Task 6.** Now go through several hyperparameters across the grid and find the optimal combinations (best average quality value) for each algorithm in this case:\n",
    "  - KNN - number of neighbors (**n_neighbors**) and metric (**metric**)\n",
    "  - DecisonTree - tree depth (**max_depth**) and partitioning criterion (**criterion**)\n",
    "  - RandomForest - the partitioning criterion in trees (**criterion**) and the maximum number of considered features (**max_features**); use the trees found earlier\n",
    "  - SGDClassifier - optimized function (**loss**) and **penalty**\n",
    " \n",
    "Please note that this operation can be resource intensive and time consuming. How to optimize the selection of parameters on the grid is described in the section \"Selection of hyperparameters of the model\".\n",
    "\n",
    "Which algorithm has the best quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert value that should be checked during the grid search of optimal parameters\n",
    "\n",
    "classifiers = {\n",
    "    KNeighborsClassifier: [{'n_neighbors': [<Student, change me>],\n",
    "                            'metric': [<Student, change me>]\n",
    "                           }],\n",
    "    DecisionTreeClassifier: [{'max_depth': [<Student, change me>], \n",
    "                              'criterion': [<Student, change me>]\n",
    "                             }],\n",
    "    SGDClassifier: [{'loss': [<Student, change me>],\n",
    "                     'penalty': [<Student, change me>]}],\n",
    "    RandomForestClassifier: [{'criterion': [<Student, change me>],\n",
    "                              'max_features': [<Student, change me>],\n",
    "                              'n_estimators': [<Put correct number here>]\n",
    "                             }]\n",
    "}\n",
    "\n",
    "trained_clfs = []\n",
    "\n",
    "for classifier, params in classifiers.items():\n",
    "    clf = GridSearchCV(classifier(), param_grid=params, cv=KFold(n_splits=5), return_train_score=True)\n",
    "    clf.fit(scaled_features, y_bin)\n",
    "    trained_clfs.append([clf, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "mean_values = []\n",
    "stds = []\n",
    "names = []\n",
    "\n",
    "for clf in trained_clfs:\n",
    "    mean_vals = clf[0].cv_results_['mean_test_score']\n",
    "    std_vals = clf[0].cv_results_['std_test_score']\n",
    "    params = clf[0].cv_results_['params'][0]\n",
    "    \n",
    "    idx = np.argmin(clf[0].cv_results_['rank_test_score'])\n",
    "    opt_params = clf[0].cv_results_['params'][idx]\n",
    "    mean = clf[0].cv_results_['mean_test_score'][idx]\n",
    "    std = clf[0].cv_results_['std_test_score'][idx]\n",
    "    name = clf[0].estimator.__str__().split('(')[0]\n",
    "    stds.append(std)\n",
    "    mean_values.append(mean)\n",
    "    names.append(name)\n",
    "\n",
    "plt.title('Performance of classifiers with the best optimized parameter value')\n",
    "plt.errorbar(np.arange(len(mean_values)), mean_values, stds, linestyle='None', marker='*')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('classifier')\n",
    "plt.xticks(np.arange(len(mean_values)), names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.5 points) Task 7.** Build for different algorithms graphics [learning curves](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html), depicting the dependence of quality on test and training samples on the number of objects on which models are trained. Look at the behavior of the curves and answer the questions:\n",
    "* Can the quality on the test sample decrease with an increase in the number of objects? And on the training? Why? \n",
    "* For what purposes can quality knowledge be used on the training part of the sample?\n",
    "* Which algorithm is better trained on fewer objects?\n",
    "* Can the addition of new objects significantly improve the quality of any of the algorithms or, with the existing data set for all algorithms, saturation occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from here: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in trained_clfs:\n",
    "    name = clf[0].estimator.__str__().split('(')[0]\n",
    "    plot_learning_curve(clf[0].best_estimator_, 'Learning Curves ({})'.format(name), scaled_features, y_bin, cv=KFold(n_splits=5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 point) Adding categorical features to models\n",
    "\n",
    "So far we have not used non-numeric attributes that are in dataset. Let's see if we did the right thing and whether the quality of the models will increase after adding these attributes.\n",
    "\n",
    "**(0.5 points) Task 8.** Transform all categorical features using the one-hot-encoding method (for example, you can do this using [pandas.get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) or [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) / [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) from sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Elevation', 'Aspect', 'Slope',\n",
       "       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
       "       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n",
       "       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Soil_Type',\n",
       "       'Wilderness_Area'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "# Select only the columns that are categorical here.\n",
    "]\n",
    "\n",
    "cat_features = X_bin[cat_columns]\n",
    "cat_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(categories='auto')\n",
    "categorical_encoded_features = enc.fit_transform(cat_features).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since after coding, signs turned out to be quite a lot, in this work we will not re-select the optimal hyperparameters for models taking into account new signs (although it would be better to do it). \n",
    "\n",
    "**(0.5 points) Task 9.** Add coded categorical to scaled real-time features and train algorithms with the best hyper-parameters found earlier. Did the addition of new signs increase the quality? Measure quality as before using a 5-fold CV. For this it is convenient to use the function [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html#sklearn.cross_validation.cross_val_score).\n",
    "\n",
    "Is the best classifier now different from the best in the previous paragraph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = np.hstack([X_bin, categorical_encoded_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<do the rest here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 points) Mixing models (blending)\n",
    "When implementing their models, it is good practice to create sklearn-compatible classes. First, such an implementation will have a standard interface and will allow other people to teach the models you have implemented without serious consequences. Secondly, it is possible to use any sklearn package functionality that accepts a model as input, for example, the class **GridSearchCV**, **learning_curve** and others.\n",
    "\n",
    "Create a classifier that is initialized with two arbitrary classifiers and the $ \\ alpha $ parameter. During training, such a classifier should train both basic models, and at the stage of prediction, knead predictions of basic models according to the formula indicated above.\n",
    "\n",
    "To create a custom classifier, you must inherit from the base classes.\n",
    "\n",
    "\n",
    "In all the preceding paragraphs, we obtained many strong models that can be quite different in nature (for example, the method of the nearest neighbors and the random forest). Often in practice it is possible to increase the quality of prediction by mixing different models. Let's see if this approach really gives an increase in quality.\n",
    "\n",
    "Choose from the constructed models of the two previous points two, which gave the highest quality on cross-validation (we denote them $ clf_1 $ and $ clf_2 $). Next, build a new classifier, whose answer on some object $ x $ will look like this:\n",
    "\n",
    "$$result(x) = clf_1(x) * \\alpha + clf_2(x) * (1 - \\alpha)$$\n",
    "\n",
    "where $ \\alpha $ is a hyper-parameter of the new classifier.\n",
    "\n",
    "**(1 point) Task 10.**\n",
    "When implementing their models, it is good practice to create sklearn-compatible classes. First, such an implementation will have a standard interface and will allow other people to teach the models you have implemented without serious consequences. Secondly, it is possible to use any sklearn package functionality that accepts a model as input, for example, the class * GridSearchCV *, * learning_curve * and others.\n",
    "\n",
    "Create a classifier that is initialized with two arbitrary classifiers and the $ \\ alpha $ parameter. During training, such a classifier should train both basic models, and at the stage of prediction, knead predictions of basic models according to the formula indicated above.\n",
    "\n",
    "To create a custom classifier, you must inherit from the base classes.\n",
    "*[BaseEstimator](http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html), [ClassifierMixin](http://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html)* and implement methods*\\_\\_init\\_\\_, fit, predict and predict_proba*. Example sklearn-compatible classifier with comments can be found [here](http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlendingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, clf1=None, clf2=None, alpha=None):\n",
    "        pass # IMPLEMENT ME\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        pass # IMPLEMENT ME\n",
    "    \n",
    "    def predict(self, x):\n",
    "        pass # IMPLEMENT ME\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        pass # IMPLEMENT ME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point) Task 11.** Select the value $ \\ alpha $ for this classifier from the grid from 0 to 1. If the class is implemented correctly, then you can use * GridSearchCV *, as is the case with conventional classifiers.\n",
    "\n",
    "Draw on the graph the average quality of the folds and the confidence interval depending on $ \\ alpha $.\n",
    "\n",
    "Did this approach increase in quality compared to models that were trained separately? Explain why even simple blending of models can influence the final quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(\n",
    "    BlendingClassifier(\n",
    "        clf1=trained_clfs[1][0],\n",
    "        clf2=trained_clfs[3][0]\n",
    "    ), param_grid=[{'alpha': np.linspace(0, 1, num=10)}],\n",
    "    cv=KFold(n_splits=5), return_train_score=True)\n",
    "\n",
    "trained = clf.fit(all_features, y_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(trained.cv_results_['param_alpha'].data.astype(np.float32), trained.cv_results_['mean_test_score'], 'b', label='accuracy')\n",
    "plt.title('Mean folds accuracy')\n",
    "plt.xlabel('x')\n",
    "plt.xlabel('alpha')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.fill_between(trained.cv_results_['param_alpha'].data.astype(np.float32), trained.cv_results_['mean_test_score'] - trained.cv_results_['std_test_score'], trained.cv_results_['mean_test_score'] +  trained.cv_results_['std_test_score'], facecolor='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Models comparison\n",
    "\n",
    "![](http://cdn.shopify.com/s/files/1/0870/1066/files/compare_e8b89647-3cb6-4871-a976-2e36e5987773.png?1750043340268621065)\n",
    "\n",
    "After many models have been built, the right continuation is to compare them with each other. At the visualization workshop, you were shown how to build a “box plot”. We use it to compare the algorithms with each other.\n",
    "\n",
    "**(1 point) Task 12.** For each type of classifier (kNN, DecisionTree, RandomForest, SGD classifier), as well as a mixed model, choose the one that gave the best quality for cross-validation and build a span diagram. All classifiers should be displayed on the same graph.\n",
    " \n",
    "Make general summary conclusions about the classifiers in terms of their work with the signs and the complexity of the model itself (what kind of hyperparameters the model has, whether changing the value of the hyperparameter greatly affects the quality of the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_clfs.append([trained])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = []\n",
    "splits = []\n",
    "stds = []\n",
    "names = []\n",
    "\n",
    "for clf in trained_clfs:\n",
    "    mean_vals = clf[0].cv_results_['mean_test_score']\n",
    "    std_vals = clf[0].cv_results_['std_test_score']\n",
    "    params = clf[0].cv_results_['params'][0]\n",
    "    \n",
    "    idx = np.argmin(clf[0].cv_results_['rank_test_score'])\n",
    "    opt_params = clf[0].cv_results_['params'][idx]\n",
    "    mean = clf[0].cv_results_['mean_test_score'][idx]\n",
    "    std = clf[0].cv_results_['std_test_score'][idx]\n",
    "    name = clf[0].estimator.__str__().split('(')[0]\n",
    "    split = []\n",
    "    for i in range(5):\n",
    "        split.append(clf[0].cv_results_['split{}_test_score'.format(i)][idx])\n",
    "    stds.append(std)\n",
    "    mean_values.append(mean)\n",
    "    names.append(name)\n",
    "    splits.append(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(data=np.asarray(splits).T, columns=[names])\n",
    "ax = results_df.boxplot(figsize=(15, 8), return_type='axes', sym='k.')\n",
    "_ = plt.setp(ax.lines, linewidth=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus task 1\n",
    "Try to find the best classifier (using the procedure above) for full dataset with all cover types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus task 2\n",
    "One with the best performance on the full dataset gets a chocolate.\n",
    "You are not allowed to use additional data.\n",
    "You are allowed to use publically available code.\n",
    "You are free to create derived features from existing ones, experiment with different methods (classes of sklearn) and even to apply deep methods here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
